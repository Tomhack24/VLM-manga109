model:
  id: "Qwen/Qwen2.5-VL-3B-Instruct"
  use_qlora: true
  device: "cuda:1"

bnb:
  load_in_4bit: True
  bnb_4bit_use_double_quant: True
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_type: "torch.bfloat16"

lora:
    lora_alpha: 16
    lora_dropout: 0.05
    r: 8
    bias: "none"   # "none"は、LoRAのバイアスを使用しないことを意味します
    target_modules: #どこをLoRAの対象にするかを指定する
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
    task_type: "CAUSAL_LM"  # タスクタイプを指定します。ここではCausal Language Modelingを使用しています      